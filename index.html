<!doctype html>
<html>
	<head>

<!-- 
    SPEAKING FEEDBACK:

    I'm running out of air while talking. Need to pause after switching
    to a big slide.

    After I bring up PADS, give people a reminder about what it is
    whenever I bring it up again.

    Give people reminders about what a capture group is.

    I don't give enough time for people to digest first sets.

    The literal highlighting of different expressions when talking
    about the literal scan optimization needs verbal crutches.

    I got a little lost describing the performance of the case study.

    If I'm going to have graphs of any kind, walk the audience though
    the graph. "Higher is worse.", "Scaling factor works this way."
-->

<!-- --------------------------------------------------------------------
                             REVEAL.JS HEADER
     -------------------------------------------------------------------- -->

		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Skip Regex</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">
        <link rel="stylesheet" type="text/css" href="css/index.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

	</head>
	<body>
		<div class="reveal">
			<div class="slides">

<!-- --------------------------------------------------------------------
                           Intro
     -------------------------------------------------------------------- -->

<section>
    <h1>Skip Regex: Parsing without Deciding</h1>
    Ethan Pailes
</section>

<section>
    <h2>Talk Roadmap</h2>
    <ol>
        <span class="fragment"><li>The Problem: Partial Parsing</li></span>
        <span class="fragment"><li>NFA Simulation Background</li></span>
        <span class="fragment"><li>Optimizations</li></span>
        <span class="fragment"><li>NFA Simulation Backends</li></span>
        <span class="fragment"><li>Evaluation</li></span>
        <span class="fragment"><li>Wrap Up</li></span>
    </ol>
    <aside class="notes">
        <ol>
            <li>I will define the problem: partial parsing</li>
            <li>I will go over nondeterminsitic finite automatons and how they are simulated.</li>
            <li>I will present my optimizations</li>
            <li>I will go over my NFA Simulation implementations</li>
            <li>I will try to convince you that my opts are worthwhile</li>
            <li>Wrap up with some related work and future directions</li>
        <ol>
    </aside>
</section>

<!-- --------------------------------------------------------------------
                           MOTIVATION
     -------------------------------------------------------------------- -->

<section id="modivation">
    <h1>Motivation</h1>

<section id="parsing-just-enough">
    <h2>Parsing Just Enough</h2>
    <div class="fragment" data-fragment-index="1">
        <pre>
BEGIN:VCARD
VERSION:3.0
N:Gump;Forrest;;Mr.;
FN:<span class="fragment highlight-green"
         data-fragment-index="2">Forrest Gump</span>
ORG:Bubba Gump Shrimp Co.
TITLE:Shrimp Man
PHOTO;VALUE=URI;TYPE=GIF: <span style="color: #9966ff;">...</span>
TEL;TYPE=WORK,VOICE: <span class="fragment highlight-red"
                           data-fragment-index="3">(111) 555-1212</span>
TEL;TYPE=HOME,VOICE: <span class="fragment highlight-red"
                           data-fragment-index="3">(404) 555-1212</span>
<span style="color: #9966ff;">...</span>
EMAIL:forrestgump@example.com
REV:2008-04-24T19:52:43Z
END:VCARD</pre>
    </div>

    <aside class="notes">
        While I ended up very focused on regex, this work started
        off because of some work that Kathleen and Nate have done
        in the context of PADS and Forest, two data description
        languages. PADS can already parse ad-hoc data formats
        pretty well, but for certain tasks data gets parsed only
        to be immediately thrown away. For example, suppose I keep
        my contact list in the standard VCARD format and I want to
        pull out the full names of everyone that I know. Using
        PADS as it is, this would involve parsing out all this extra
        information like phone numbers and then just dropping it on
        the floor. We wanted to do as little as possible to get that
        name.
    </aside>
</section>

<section id="tree-slide1">
    <div style="height: 500px; width: 1000px;"
         class="fig-container"
         data-file="fig/parse-tree-subset1.html">
    </div>

    <aside class="notes">
        Here is a partial parse tree for that VCARD data I just
        showed you.
    </aside>
</section>

<section id="tree-slide2">
    <div style="height: 500px; width: 1000px;"
         class="fig-container"
         data-file="fig/parse-tree-subset2.html">
    </div>

    <aside class="notes">
        In an ideal world we would have an oracle that tells us where
        we have to go in order to get the interesting bits.
    </aside>
</section>

<section id="tree-slide3">
    <div style="height: 500px; width: 1000px;"
         class="fig-container"
         data-file="fig/parse-tree-subset3.html">
    </div>

    <aside class="notes">
        Obviously this is impossible. We are going to have to look
        at some other parts of the data to figure out where what
        we are interested in begins and ends.
    </aside>
</section>

<section id="tree-slide4">
    <div style="height: 500px; width: 1000px;"
         class="fig-container"
         data-file="fig/parse-tree-subset4.html">
    </div>

    <aside class="notes">
        In fact, at first it seems worse than that. What if the input
        is ill formed? It seems like we have to perform a full parse
        no matter what. We can never do anything clever to skip over
        bits of the input if we are required to check it for validity.
    </aside>
</section>

<section id="impossible">
    <h2>Partial Parsing is Impossible
    </h2>

    <aside class="notes">
        You can't skip any part of the input if you hope to validate it.
        Time to give up.
    </aside>
</section>

<section id="possible">
    <h2>Partial Parsing is Possible*</h2>

    <aside class="notes">
        In order to get out of this predicament, I'm going to tack
        on an asterisk and shift the goal posts. Later
        I'm going to explain why that shift is actually very
        reasonable.
    </aside>
</section>

<section id="asterix">
    <h2>*: The Partial Parsing Assumption
    </h2>

    <p class="fragment">
        Given a grammar, g, a subset, s, of g to extract and an input
        <i>in the language of g</i>, a partial parse extracts the part
        of the input corresponding to s.
    </p>

    <aside class="notes">
    </aside>
</section>

<section id="tree-slide5">
    <div style="height: 500px; width: 1000px;"
         class="fig-container"
         data-file="fig/parse-tree-subset3.html">
    </div>

    <aside class="notes">
        Now if we go back to the partial parse tree we have
        a hope of skipping over those yellow bits.
    </aside>
</section>

<section id="partial-regex">
    <h2>Partially Parsing Regex</h2>
    <ul>
        <li>Constrained Semantics</li>
        <li>A Strict Subset of PADS</li>
        <li>Built-In Notion of a Partial Parse</li>
        <li>Friendly Performance Quirks</li>
    <ul>

    <aside class="notes">
        Regex make a great place to start investigating partial
        parsing. They have a more constrained semantics than PADS,
        they already have capture groups which encode a desired
        subset of the input to extract, and they are a part of
        PADS so we were going to have to deal with them anyway.
    </aside>
</section>

<section>
    <h2>Built-In Partial Parse</h2>

    <p>/a*(c{1,3})x$/</p>

    <p>aaaaaa<span class="fragment highlight-green">cc</span>x</p>

    <aside class="notes">
        A capture group asks for us to report a particular subsection
        of matching text. This is pretty much exactly what I mean
        when I say a subset of the grammar to be partially parsed.
    </aside>
</section>

<section>
    <h2>Friendly Performance Quirks</h2>

    <!-- TODO: introduce automata first. -->

    <ul>
        <li class="fragment">DFAs can't easily perform submatch extraction.</li>
        <li class="fragment">NFAs Simulation &lt;&lt; DFA Execution &lt;&lt; Substring Search</li>
    <ul>

    <aside class="notes">
        I want to explain what I mean when I say that regular expression
        parsing has some performance quirks that are friendly to partial
        parsing. Deciding and parsing regular expression happens at very
        different speeds. In fact, because of the relationship between
        all the different problems in play in a regex engine, a DFA is
        often used to find matching text even before an NFA is executed.

        To see why this makes sense, imagine trying to extract all the
        arguments to a particular C++ template class with a regex
        capture group. Most of the source code you search over won't
        contain that template class, so running the NFA is a waste.
        The DFA is already good enough to discard those lines. You only
        need the NFA for the small subset of lines which actually match.

        This means that skip regex can actually provide a performance
        improvement, even when we require the input to be decided!
    </aside>
</section>

<section>
    <h2>Optimization Goals</h2>

    <!-- TODO: Use pictures/animations here instead. -->

    <ul>
        <li class="fragment">Find places to skip.</li>
        <li class="fragment">Find places to scan.</li>
    <ul>

    <aside class="notes">
        This leaves us in a place where we are looking to optimize
        submatch extraction by either just wholesale skipping forward
        over the input, or finding places where we can recast part of
        the problem in terms of substring search.
    </aside>
</section>

</section>

<!-- --------------------------------------------------------------------
                           NECESSARY BACKGROUND
     -------------------------------------------------------------------- -->

<section id="background">
    <h1>Background</h1>
<!-- FALLTHROUGH -->


<section id="background-real">
    <ul>
        <li class="fragment">Non-deterministic Finite Automata</li>
        <li class="fragment">Virtual Machines</li>
        <li class="fragment">Compiling Regex</li>
    </ul>

    <aside class="notes">
        Before I can tell you about all the cool ways that we can
        take advantage of that asterisks, I'm going to take a step
        back and tell you about how NFA simulations are implemented.
    </aside>
</section>

<section id="nfas">
    <p>Non-Deterministic Finite Automata</p>
    <img src="./fig/NFAexample.svg" height="400px;">

    <aside class="notes">
        <p>
        Very quickly, any regular expression has a corresponding
        nondeterministic finite automata which decides its language.
        An NFA is a directed graph with annotated edges that indicate
        how we should transition between states on various input.
        It also includes so called epsilon edges which we follow
        immediately whenever we see them. What makes it non-determinisitic
        is that the execution splits whenever there are multiple transitions
        on one character or an epsilon edge. Put another way, whenever
        we get to one of those points we just copy the whole NFA and keep
        executing the two versions simultaneously.
        </p>

        <p>
        This NFA accepts strings with either an even number of zeros or
        an even number of ones.
        </p>
    </aside>
</section>

<section id="nfavms">
    <h2>NFA VMs</h2>

    <pre>
<span class="fragment highlight-green"
              data-fragment-index="1"><span
                class="fragment highlight-black"
              data-fragment-index="2">save 0</span></span>
<span class="fragment highlight-green"
              data-fragment-index="2"><span
                class="fragment highlight-black"
              data-fragment-index="3"><span
                class="fragment highlight-green"
              data-fragment-index="4"><span
                class="fragment highlight-black"
              data-fragment-index="5"><span
                class="fragment highlight-green"
              data-fragment-index="6">L1: char 'a'</span></span></span></span></span>
<span class="fragment highlight-green"
              data-fragment-index="3"><span
                class="fragment highlight-black"
              data-fragment-index="4"><span
                class="fragment highlight-green"
              data-fragment-index="5"><span
                class="fragment highlight-black"
              data-fragment-index="6">split L1 L2</span></span></span></span>
<span class="fragment highlight-green"
              data-fragment-index="4"><span
                class="fragment highlight-black"
              data-fragment-index="5"><span
                class="fragment highlight-green"
                data-fragment-index="6">L2: char 'b'</span></span></span>
<span class="fragment highlight-green"
              data-fragment-index="5"><span
                class="fragment highlight-black"
              data-fragment-index="6">save 1</span></span>
<span class="fragment highlight-green"
              data-fragment-index="6">match</span></pre>

    <aside class="notes">
        <p>
        I'm going to immediately move away from that abstract notion of
        NFAs that I just showed you and which you have probably seen
        in a computational theory course. Instead I'm going to talk
        to you about an NFA VM.
        </p>

        <p>
        In order to implement regex we use a special VM that
        has special character matching instructions and support
        for non-determinism built right in. The VM executes a number
        of threads which each have some instruction pointer and some
        string pointer. I'm about to explain what all these instructions
        do, but for now I just want you to get a feel for how multiple
        threads of execution are simultaneously moving through different
        points in the program.
        </p>
    </aside>
</section>

<section id="base-instructions">
    <table style="float: left;">
        <tr>
            <td><pre>save n</pre></td>
            <td>Save the current string index in slot n.</td>
        </tr>
        <tr>
            <td><pre>char a</pre></td>
            <td>Kill thread if input[string index] is not in a.</td>
        </tr>
        <tr>
            <td><pre>jmp L</pre></td>
            <td>Goto label L.</td>
        </tr>
        <tr>
            <td><pre>split L1 L2</pre></td>
            <td>Goto L1, spawn a new thread at L2.</td>
        </tr>
        <tr>
            <td><pre>match</pre></td>
            <td>We made it!</td>
        </tr>
    </table>

    <aside class="notes">
        This should make what was going on in that last slide a little
        clearer. These instructions are all you need to execute conventional
        regex, and I'm going to show you how to do that right now.
    </aside>
</section>

<section id="compiling">
    <div class="codefunc">
    <table style="float: right;">
        <tr>
            <td>code((e))</td>
            <td>
                <pre>
save n
code(e)
save (n+1)</pre>
            </td>
        </tr>
        <tr>
            <td>code(e1 e2)</td>
            <td>
                <pre>
code(e1)
code(e2)</pre>
            </td>
        </tr>
        <tr>
            <td>code(e1|e2)</td>
            <td>
                <pre>
split L1 L2
L1: code(e1)
jmp L3
L2: code(e2)
L3:</pre>
            </td>
        </tr>
    </table>

    <table style="float: left;">
        <tr>
            <td>code(a)</td>
            <td>
                <pre>char a</pre>
            </td>
        </tr>
        <tr>
            <td>code(e*)</td>
            <td>
                <pre>
split L1 L2
L1: code(e)
split L1 L2
L2:</pre>
            </td>
        </tr>
        <tr>
            <td>code(e+)</td>
            <td>
                <pre>
L1: code(e)
split L1 L2
L3:</pre>
            </td>
        </tr>
        <tr>
            <td>code(.)</td>
            <td>
                <pre>char SIGMA</pre>
            </td>
        </tr>
    </table>
    </div>

    <aside class="notes">
        Once you've defined the VM, the translation from regex
        to instructions just sort of falls out. If you want to
        learn more about this stuff Russ Cox has a really great
        blog post. Note that capture groups are compiled to use
        the save instruction. That n is a counter which increments
        by 2 for ever capture group. Also note that the split instruction
        is sufficient to deal with all of the non-determinism
        inherent in regex.
    </aside>
</section>

<section id="apply-code0">
    <p>code(a*b(c|d))</p>

    <pre>
<span style="color: green;">code(a*b(c|d))</span></pre>

    <aside class="notes">
        To make that a little more concrete, I'm going to step though an
        example.
    </aside>
</section>

<section id="apply-code1">
    <p>code(a*b(c|d))</p>

    <pre>
code(a*)
<span style="color: green;">code(b(c|d))</span></pre>
</section>

<section id="apply-code2">
    <p>code(a*b(c|d))</p>

    <pre>
<span style="color: green;">code(a*)</span>
code(b)
code((c|d))</pre>
</section>

<section id="apply-code3">
    <p>code(a*b(c|d))</p>

    <pre>
split L1 L2
L1: <span style="color: green;">code(a)</span>
split L1 L2
L2:
code(b)
code((c|d))</pre>
</section>

<section id="apply-code4">
    <p>code(a*b(c|d))</p>

    <pre>
split L1 L2
L1: char a
split L1 L2
L2:
<span style="color: green;">code(b)</span>
code((c|d))</pre>
</section>

<section id="apply-code5">
    <p>code(a*b(c|d))</p>

    <pre>
split L1 L2
L1: char a
split L1 L2
L2:
char b
<span style="color: green;">code((c|d))</span></pre>
</section>

<section id="apply-code6">
    <p>code(a*b(c|d))</p>

    <pre>
split L1 L2
L1: char a
split L1 L2
L2:
char b
save 2
<span style="color: green;">code(c|d)</span>
save 3</pre>
</section>

<section id="apply-code7">
    <p>code(a*b(c|d))</p>

    <pre>
split L1 L2
L1: char a
split L1 L2
L2:
char b
save 2
split L3 L4
L3: <span style="color: green;">code(c)</span>
jmp L5
L4: <span style="color: green;">code(d)</span>
L5:
save 3</pre>
</section>

<section id="apply-code8">
    <p>code(a*b(c|d))</p>

    <pre>
split L1 L2
L1: char a
split L1 L2
L2:
char b
save 2
split L3 L4
L3: char c
jmp L5
L4: char d
L5:
save 3</pre>
</section>

<section id="apply-code9">
    <p>code(a*b(c|d))</p>

    <pre>
save 0
split L1 L2
L1: char a
split L1 L2
L2:
char b
save 2
split L3 L4
L3: char c
jmp L5
L4: char d
L5:
save 3
save 1</pre>
</section>

</section> <!-- background -->

<section id="skip-instructions">
    <h2>Extension: Skip Instructions</h2>
    <table style="float: left;">
        <tr>
            <td><pre>skip n</pre></td>
            <td>Skip forward in the input by n.</td>
        </tr>
        <tr>
            <td><pre>scan-begin lit</pre></td>
            <td>Scan forward to lit and stop at the beginning.</td>
        </tr>
        <tr>
            <td><pre>scan-end lit</pre></td>
            <td>Scan forward to lit and stop at the end.</td>
        </tr>
        <tr>
            <td><pre>goto-end</pre></td>
            <td>Just go to the end of the input.</td>
        </tr>
    </table>

    <aside class="notes">
        <p>
        I had to add a few extra instructions in order to support the
        optimizations that I implemented. They all enable us to avoid
        looking at some part of the input or look at it a lot faster.
        One thing that I didn't know going into this is that substring
        search is a lot faster than even regex DFAs. There are sub-linear
        algorithms to get it done really really fast, and special
        hardware to get it done even faster than that.
        </p>

        <p>
        Figuring out how to use these new instructions is a bit
        more complex than compiling to the standard set of instructions.
        I'm going to spend the rest of my talk doing that.
        </p>
    </aside>
</section>

<!-- --------------------------------------------------------------------
                           .* Opt
     -------------------------------------------------------------------- -->

<section id="dotstar-opt">
<h1>.*l Optimization</h1>

<section id="dotstar-why">
    <table>
        <tr>
            <td>Spawning threads is expensive</td>
            <td>
                <pre class="fragment highlight-red">
split L1 L2
L1: char SIGMA
split L1 L2
L2: char 'f'
char 'o'
char 'o'
</pre></td>
        </tr>

        <tr>
            <td>Scanning forward is cheap</td>
            <td><pre class="fragment highlight-green">scan-end 'foo'</pre></td>
        </tr>
    </table>

    <aside class="notes">
        <p>
        The first optimization I'm going to talk about is
        optimizing .* followed by a literal. When I write regular expressions,
        I very frequently find myself writing "match anything,
        stop when you see this". The problem is that that .*
        involves spawning a new thread <i>for every character
        of input</i>.
        </p>

        <p>
        So we want to turn this into this.
        </p>
    </aside>
</section>

<section id="dotstar-problem">
    <h1>Problem!</h1>
    <div style="float: left;">
    /.*foox+/
    </div>

    <div style="float: right;">
    <span class="fragment fade-out"
        data-fragment-index="3">aaaaa</span><span class="fragment highlight-red"
            data-fragment-index="1">foo</span>ooooooo<span
        class="fragment highlight-green"
        data-fragment-index="2">foo</span>xxxxx
    </div>

    <div style="padding-top: 150px;">
    <pre>
<span class="fragment" data-fragment-index="2">L1: </span>scan-end 'foo'
<span class="fragment" data-fragment-index="2">split L1 L2
L2:</span></pre>
    <div>

    <aside class="notes">
        <p>
        There is a problem with just compiling straight to scan-end.
        What if there is an occurrence of foo in the part of the input
        that is supposed to match the .*? This example should clearly
        match, but if we just compile straight to a scan instruction
        it wont. The solution is to add a non-deterministic split.
        </p>

        <p>
        Now if you've been paying attention, you might have noticed
        that there is still one less split instruction than what
        we had before. That's ok because the single split instruction
        will handle the case where there are zero repeated elements
        just fine.
        </p>
    </aside>
</section>

</section> <!-- dotstar-opt -->

<!-- --------------------------------------------------------------------
                           el Opt
     -------------------------------------------------------------------- -->

<section id="elit-scan">

<section id="elit-why">
    <h2>Literal Scan Optimization</h2>

    <span class="fragment">/<span
        class="fragment highlight-red">(?:we|are|having|a| )*</span>blast/</span>

    <pre class="fragment">scan-end 'blast'</pre>
    
    <aside class="notes">
        <p>
        Personally, I think this is the coolest optimization I
        have to share with you today. Our goal is pretty much the
        same as it was for the .* optimization, but this time
        we are going to get to throw out that non-determinism
        entirely.
        </p>

        <p>
        This regex is an example of one that would benefit from
        this optimization. That weird question mark colon syntax is
        how you ask rust regex for a non-capturing grouping.
        Starting off the regex we have an
        expensive expression with two different sources of
        non-determinism. We could decide it, but it would
        involve all sorts of thread juggling. But notice
        that "blast" can never appear in that leading expression.
        So we could just implement it with this code.
        </p>
    </aside>
</section>

<section id="elit-but-how">
    <h2>When is it okay to do this?</h2>

    <p class="fragment">l &notin; L(/e/)</p>
    <p class="fragment">L(/e/) &cap; L(/.*l.*/) = &empty;</p>

    <aside class="notes">
        The key to figuring out when it is ok to do this is
        our ability to decide this question NEXT. Restated in
        terms of set intersection that question becomes NEXT.
    </aside>
</section>

<section id="elit-lang-intersection1">

    <!-- TODO: drop or shorten this. I just need one slide. Just
                say we construct an NFA from the NFAs of the sublanguages.
                Don't go into any detail. -->

    <p>L(/aa/) &cap; L(/ab/) &#x225f; &empty;</p>

    <div class="fragment">
    <img src="./resources/aa-nfa.png" />
    <img src="./resources/ab-nfa.png" />
    </div>

    <aside class="notes">
        I want to consider this intersection problem a bit more generally,
        so we are actually just going to focus on determining if the intersection
        of two different regular languages is empty. Each regular language
        has a corresponding NFA NEXT. For these two languages the NFAs
        are pretty simple. We can take the intersection of them by
        constructing a new NFA where the state set is the Cartesian
        product of the state sets of the two NFAs we want to intersect,
        then adding transitions only where both could make progress.
    </aside>
</section>

<section id="elit-lang-intersection2-s1">
    <p>L(/aa/) &cap; L(/ab/) &#x225f; &empty;</p>

    <img style="height: 400px" src="./resources/aa-intersect-ab-step1.png" />

    <aside class="notes">
        To construct an NFA which decides the intersection of two
        NFAs we first create a set of states by taking the cartesian
        product of the set of states of the two input NFAs.
    </aside>
</section>

<section id="elit-lang-intersection2-s2">
    <p>L(/aa/) &cap; L(/ab/) &#x225f; &empty;</p>

    <img style="height: 400px" src="./resources/aa-intersect-ab-step2.png" />

    <aside class="notes">
        Now, we add edges whereever both NFAs could make
        progress. Can we ever transition from state 0
        in the 'ab' machine to state 1 while remaining
        in state 0 for the 'aa' machine?
    </aside>
</section>

<section style="height: 400px" id="elit-lang-intersection2-s3">
    <p>L(/aa/) &cap; L(/ab/) &#x225f; &empty;</p>

    <img src="./resources/aa-intersect-ab-step3.png" />

    <aside class="notes">
        No we can't. How about transitioning from state
        0 0 to state 1 1. Both can make progress on
        an 'a', so...
    </aside>
</section>

<section style="height: 400px" id="elit-lang-intersection2-s4">
    <p>L(/aa/) &cap; L(/ab/) &#x225f; &empty;</p>

    <img src="./resources/aa-intersect-ab-step4.png" />

    <aside class="notes">
        That works.
    </aside>
</section>

<section style="height: 400px" id="elit-lang-intersection2-s5">
    <p>L(/aa/) &cap; L(/ab/) &#x225f; &empty;</p>

    <img src="./resources/aa-intersect-ab-step5.png" />

    <aside class="notes">
        Can we make it to the accepting state?
    </aside>
</section>

<section style="height: 400px" id="elit-lang-intersection2-s6">
    <p>L(/aa/) &cap; L(/ab/) &#x225f; &empty;</p>

    <img src="./resources/aa-intersect-ab-step6.png" />

    <aside class="notes">
        No.
    </aside>
</section>

<section id="elit-lang-intersection2">
    <p class="fragment highlight-green">L(/aa/) &cap; L(/ab/) &#x225f; &empty;</p>

    <img src="./resources/aa-intersect-ab.png" />

    <aside class="notes">
        Now I'll prune off the orphan states. This NFA obviously can
        never match, so its language is the empty set.
    </aside>
</section>

<section id="elit-lang-intersection3">
    <p>L(/aa/) &cap; L(/a*/) &#x225f; &empty;</p>

    <img src="./resources/aa-nfa.png" />
    <img src="./resources/astar-nfa.png" />

    <aside class="notes">
        Here is a slightly more complicated example
    </aside>
</section>

<section id="elit-lang-intersection4">
    <p class="fragment highlight-red">L(/aa/) &cap; L(/a*/) &#x225f; &empty;</p>

    <img src="./resources/aa-intersect-astar.png" />

    <aside class="notes">
        And as you can see we can trace a path from the start state to
        the end state, so the two languages intersect.
    </aside>
</section>

<section id="elit-biggest1">
    /<span
    class="fragment highlight-green"
    data-fragment-index="1"><span
    class="fragment highlight-black"
    data-fragment-index="2">lit0</span></span> e ... <span
    class="fragment highlight-green"
    data-fragment-index="2"><span
    class="fragment highlight-black"
    data-fragment-index="3">e lit1</span></span>e ... <span
    class="fragment highlight-green"
    data-fragment-index="3"><span
    class="fragment highlight-black"
    data-fragment-index="4">e lit2</span></span> e ... <span
    class="fragment highlight-green"
    data-fragment-index="4"><span
    class="fragment highlight-black"
    data-fragment-index="5">e litn</span></span>/

    <aside class="notes">
        So that's the key piece of analysis we have to do in order to
        apply this optimization, but there is still quite a bit
        of leeway about how we go about doing so. The most obvious
        way is to step though the literals in order, trying to
        optimize each one in turn.
    </aside>
</section>

<section id="elit-biggest2">
    /<span
    class="fragment highlight-green"
    data-fragment-index="1"><span
    class="fragment highlight-black"
    data-fragment-index="2">lit0</span></span><span
    class="fragment highlight-green"
    data-fragment-index="2"><span
    class="fragment highlight-black"
    data-fragment-index="4"><span
    class="fragment highlight-black"
    data-fragment-index="3">e ...</span> e lit1</span></span><span
    class="fragment highlight-green"
    data-fragment-index="4"><span
    class="fragment highlight-black"
    data-fragment-index="6"><span
    class="fragment highlight-black"
    data-fragment-index="5">e ...</span> e lit2</span></span><span
    class="fragment highlight-green"
    data-fragment-index="6"><span
    class="fragment highlight-black"
    data-fragment-index="8"><span
    class="fragment highlight-black"
    data-fragment-index="7">e ...</span> e litn</span></span>/

    <aside class="notes">
        But if we just do that we might miss out on skipping over
        a bigger expression. It would be better if we looked at
        larger chunks of expressions first, then kept trying
        with smaller and smaller chunks.
    </aside>
</section>

<section id="elit-biggest3">
    /<span class="fragment highlight-green"
        data-fragment-index="1"
    ><span
        class="fragment highlight-black"
        data-fragment-index="2"
    >lit0</span><span
        class="fragment highlight-black"
        data-fragment-index="3">e</span><span
        class="fragment highlight-black"
        data-fragment-index="4">... e lit1</span><span
        class="fragment highlight-black"
        data-fragment-index="5">e ... e <span
        class="fragment highlight-green"
        data-fragment-index="6">lit2</span> e ... e</span><span
        class="fragment highlight-black"
        data-fragment-index="6">litn</span></span>/

    <aside class="notes">
        Of course that still misses the case where one literal
        can actually skip over another. To do better we have to
        start from the back and try the whole preceding expression,
        then keep walking back. Only once we've tried the whole range
        of possibilities can we move on to the next literal.
    </aside>
</section>

</section> <!-- elit-scan -->

<section id="lit-skip">

<section id="lit-skip-why">
    <h2>Literal Skip Optimization</h2>

    <span class="fragment">/foo/</span>

    <pre class="fragment">skip 3</pre>
    
    <aside class="notes">
        The idea behind this optimization is probably the simplest
        of the optimizations that I'm going to show you. If you have
        a regex with a statically knowable length, you can just compile
        it to a skip instruction. This lets us parse certain regex in
        constant time!
    </aside>
</section>

<section id="lit-skip-problem1">
    <h2>What about |</h2>

    <span class="fragment">/foo(.)|bar(.)/</span>
    &nbsp;
    &nbsp;
    &nbsp;
    &nbsp;
    &nbsp;
    &nbsp;
    <span class="fragment">barx</span>

    <table>
        <tr>
            <td class="fragment">
                <pre>
save 0
split L1 L2
L1: skip 3
save 2
skip 1
save 3
L2: ...
save 1
match</pre>
                
                <span class="fragment highlight-red">captures[1] == x</span>
            </td>
            <td class="fragment">
                <pre>
save 0
split L1 L2
L1: char 'f'
...
L2: <span class="fragment fade-out">char 'b'
skip 2</span>
<span class="fragment">skip 3</span>
save 4
skip 1
save 5
save 1
match</pre>
                
                <span class="fragment highlight-green">captures[2] == x</span>
            </td>
        </tr>
    </table>

    <aside class="notes">
        <p>
        Unfortunatly the story is not quite that simple. If a literal
        begins one of the branches of an alternative expression, just
        skipping forward will not be enough. If you look at this example
        you can see that just blindly skipping forward by 3 to skip
        over "foo" will cause the "x" to get captured in the wrong slot.
        </p>

        <p> To fix this we can emit a charicter test at the front of
        each of the literals. This will ensure than any threads which
        might lead to an incorrect match will die before they manage
        it. Actually, by the time we get to the last branch we know
        that it has to match by process of elimination, so we don't
        have to emit that charicter test. We can just go right into
        the skip. And that will give us an x in the second capture group,
        which is right.
        </p>
    </aside>
</section>

<section>
    <h2>What about *</h2>

    e1*e2*...en*eterm

    <aside class="notes">
        This concern about compiling literals to skips shows up
        any time there is a branch in the VM program, and repetitions
        are the other way to introduce branching. You have to do
        simillar sort of analysis, using the bodies of different
        repetitions instead of the different arms of an
        alternative.
    </aside>
</section>

<section id="lit-skip-problem2.1">
    <h2>What if you can't tell with a char test?</h2>

    <span class="fragment">apples|alex</span>

    <aside class="notes">
        Sometimes you can't tell the difference between two
        branches just by issuing a single char instruction. To identify
        these cases we check if the expressions in the arms of the alternative
        have intersecting first sets.
    </aside>
</section>

<section id="fsets">
    <h2>Short Digression: First Sets</h2>

    <table>
        <tr>
            <td>fset(a)</td>
            <td>{a}</td>
        </tr>
        <tr>
            <td>fset(e1|e2)</td>
            <td>fset(e1) &cup; fset(e2)</td>
        </tr>
        <tr>
            <td>fset(e*)</td>
            <td>fset(e)</td>
        </tr>
        <tr>
            <td>fset(.)</td>
            <td>&Sigma;</td>
        </tr>
        <tr>
            <td>fset((e))</td>
            <td>fset(e)</td>
        </tr>
    </table>

    <aside class="notes">
        The first set of any regular expression is the set of characters
        which might begin it. You can compute the first set
        recursively is really natural.
    </aside>
</section>

<section id="lit-skip-problem2.2">
    <h2>What if you can't tell with a char test?</h2>

    apples|alex

    <br>

    <span class="fragment">a(?:pples|lex)</span>

    <aside class="notes">
        <p>
        Now we can tell that one char instruction won't be enough
        if the first sets of any of the branches intersect. Then
        we give up.
        </p>

        <p> I'd like to point out that this particular expression is
            not totally hopeless. We could re-write it in a form that
            would not require us to give up. This sort of re-writing
            is one thing that I would want to spend more time on if
            I had it, but for now we just do the safe thing.
        </p>
        <!--
        If
        one of the first sets contain epsilon we need to examine
        the next instruction that comes along.
        -->
    </aside>
</section>

</section>

<section id="backends">

<section id="skip-backtracker">
    <h2>A Backend: The Skip Backtracker</h2>

    <ul>
        <li>One Copy of the Capture Slots</li>
        <li>Prevents Exponential Blowup with a Bitset</li>
    </ul>

    <aside class="notes">
        <p>
        I wrote two different backends to execute the code, but
        the one I ended up relying on the most was called the skip
        backtracker. Most regular expressions are implemented using
        a backtracking NFA simulation. Backtracking implementations
        are quite fast because you can keep just a single copy of
        the capture slots. Rob Pike's extension of Thomson's original
        implementation requires a separate copy of the slots for each
        thread. Juggling these capture slots can get expensive.
        </p>
        <p>
        Many people know that common regular expression implimentations
        suffer from so called "pathological cases" where the engine
        takes a huge amount of time to complete. What many people don't
        realize is that this is completely avoidable so long as you give
        up back-references. If you don't support back-references, a
        thread's history does not matter, and it is uniquely identified
        by the tuple of its string pointer and its instruction pointer.
        Then you can keep a bitset indicating all the states that the
        execution can possibly be in, and just never enter the same
        state twice.
        </p>
    </aside>
</section>

<section id="skip-pikevm">
    <h2>A Backend: The Skip Pike VM</h2>

    <ul>
        <li>Sometimes faster when the regex and input are large.</li>
        <li>Can't handle greedy vs lazy disambiguation.</li>
    </ul>

    <aside class="notes">
        <p>I'm not going to say much about my Pike VM implementation,
           because it is both fairly complex and rarely useful. It basically
           just runs a collection of threads in a window over the
           input, and enforces the fact that all live threads fall in
           that window. The technology is pretty interesting, so if you
           want to learn more, you can come talk to me afterwards.
        </p>
    </aside>
</section>

</section>

<section id="microbenchmarks">

<section id="microbenchmarks-intro">
    <h2>Micro Benchmarks</h2>

    <aside class="notes">
        <p>
        I'm now going to try to provide a performance picture.
        The optimizations I implemented can provide a speedup
        of anything from 0 percent to a million percent, so
        I don't want to give you a single summary number.
        </p>

        <p>
        Microbenchmarks are not a good way to develop a comprehensive
        performance picture, but they are excellent for illustrating
        performance edge cases. I'm going to use them to highlight
        the best and worst cases for skip regex.
        </p>
    </aside>
</section>

<section id="microbenchmarks-a-big-skip">
    <h2>A Big Skip</h2>

    /<span style="color: green;">aaaa</span>(bbbb)<span style="color: green;">cccc</span>/ &nbsp; &nbsp;
    <span style="color: green;">aaaa</span>bbbb<span style="color: green;">cccc</span>

    <br>

    <img src="./resources/a-big-skip.png">

    <aside class="notes">
        <p>
        All the microbenchmarks I'm going to show you will have a
        regular expression template and some input template. Some
        portion of the input will be repeated according to a scaling
        factor. I've highlighted the repeating section in green.
        </p>

        <p>
        This particular benchmark is showing how the literal skip
        optimization can result in constant time operation. I
        believe the slight curve to the backtracker performance
        comes from the fact that smaller bitsets are cheaper to
        initialize.
        </p>
    </aside>
</section>

<section id="microbenchmarks-leading-dotstar">
    <!-- TODO: make the benchmark ordering have a static order
               rather than being based off of speed. -->
    <h2>Leading .*</h2>

    /.*(aaaa)/ &nbsp; &nbsp; <span style="color: green;">b</span>aaaa

    <br>

    <img src="./resources/leading-dotstar.png">

    <aside class="notes">
        <p>
        This benchmark is showing the best case scenario for
        the .* optimization. It is going to find the terminator
        and will only have to pay the cost of a single branch
        back to retry.
        </p>
    </aside>
</section>

<section id="microbenchmarks-dotstar-bounce">
    <h2>.* Bounce</h2>

    /.*a(bbbb)/ &nbsp; &nbsp; <span style="color: green;">ca</span>bbbb

    <br>

    <img src="./resources/dotstar-bounce.png">

    <aside class="notes">
        <p>
        This benchmark is showing the worst case scenario for
        the .* optimization. Here the literal terminator is
        "a". Longer terminators are better, so this is already
        bad. Much worse is the fact that every two characters
        of input, a new thread will have to be split off, and
        each one of these new threads has to context switch between
        the NFA sim inner loop and the substring searcher.
        </p>
        <p>
        This scenario seems much less likely than the last one
        to me, but that's just a sort of anecdotal intuition.
        </p>
    </aside>
</section>

<section id="microbenchmarks-leading-estar-scan">
    <h2>Leading e*</h2>

    /a*foo(bar)/ &nbsp; &nbsp; <span style="color: green;">a</span>foobar

    <br>

    <img src="./resources/leading-estar.png">

    <aside class="notes">
        <p>
        This benchmark shows the best case for the el optimization.
        I won't have a worst case for you because we always statically
        prove that we will find the terminator. The numbers for those
        skip backends are actually increasing, but substring search is
        so much faster than an NFA simulation that you can't really see it.
        </p>
    </aside>
</section>

<section id="microbenchmarks-trailing-dotstar">
    <h2>Leading el</h2>

    /(a+).*/ &nbsp; &nbsp; <span style="color: green;">ab</span>

    <br>

    <img src="./resources/aplus-trailing.png">

    <aside class="notes">
        <p>
        This shows the benefits of the goto-end instruction. Note
        that the "a" and "b" are repeated separately not as a group.
        </p>
    </aside>
</section>

<section id="microbenchmarks-no-opt">
    <h2>No Opt</h2>

    /(ab|ac)*/ &nbsp; &nbsp; <span style="color: green;">ab</span>

    <br>

    <img src="./resources/no-opt.png">

    <aside class="notes">
        <p>
        This shows a case that can't be optimized. It shows some slight
        differences in the constant factors of the different engines.
        </p>
        <p>
        The difference in perf for the Pike VMs is probably due to the
        more complicated thread juggling logic I had to implement. The
        backtrackers are much closer.
        </p>
    </aside>
</section>
</section> <!-- microbenchmarks -->

<section id="case-study">
<section id="kafka-wat">
    <h2>Kafka Diagnostics: A Case Study</h2>

    <aside class="notes">
        <p>
        The microbenchmarks I just showed you should help
        outline the outside of the performance envelope,
        but now I want to go into a more realistic scenario.
        Apache Kafka is a distributed queueing system, and
        like most network applications, it has grown up
        with integrated logging. I wanted use skip regex to
        pull out some summary information about a kafka cluster
        that I set up.
        </p>
        <p>
        To generate data to work with I stood up a Kafka cluster
        with a single "test" topic, set it to log as promiscuously
        as possible and used a simple python script
        to write and read a bunch of messages. The script is on github
        along with all the code for this case study for those of you
        who are interested.
        </p>
    </aside>
</section>

<section id="append-events">
    <h2>Summarizing Append Events</h2>

    <pre class="fragment">[2018-02-15 11:39:30,073] TRACE Appended message
set to log test-0 with first offset: 1000, next offset: 1001, and
messages: [(record=DefaultRecord(offset=1000, timestamp=-1,
key=0 bytes, value=14 bytes))] (kafka.log.Log)</pre>

    <pre class="fragment">/^.* with first offset: ([0-9]+).*value=([0-9]+).*$/</pre>

    <aside class="notes">
        <p>
        One of the core operations you can perform on a Kafka
        queue is appending a message. Every time something is appended
        to a queue, it emits a line like this one. I wanted to scrape
        these lines and figure out the range of offsets that had
        been used as well as the total number of bytes appended to
        the log. I used this regex to pull out the offset and number of
        bytes.
        </p>
        <p>
        One thing thats worth noting is that I used skip regex in
        pre-validated mode here, so there was zero difference from
        a usage perspective. I just selected the skip backend and
        used it in exactly the same way I normally would have. I applied
        the regex, then if it matched parsed the two capture groups
        and updated a current min and max offset seen as well as a
        total number of bytes appended.
        </p>
    </aside>
</section>

<section id="append-graph">
    <h2>Performance on 25M of Input</h2>

    <div style="display: flex; align-items: center; ">
        <div style="float: left;">
            <img height="400px" src="./resources/append.png">
        </div>

        <ul style="float: right; font-size: 20pt;">
            <li class="fragment">26.03% improvement in user time.</li>
            <li class="fragment">Only 3.51% of lines matched</li>
        </ul>
    </div>

    <aside class="notes">
        <p>
        When I ran this summary task on 25M of input I saw a roughly
        25 percent improvement in performance, and this was despite
        the fact that a very low number of lines matched. This should
        not be too surprising considering that Amdahl's Law tells us
        that improvements to the slowest part of the system result
        in the biggest speed improvements. The DFA is fast, so
        improvements to the NFA have a disproportionate impact.
        </p>
    </aside>
</section>

<section id="append-graph-just-append">
    <h2>Performance on 848K of Matching Input</h2>

    <img height="400px" src="./resources/append-just-match.png">

    <aside class="notes">
        <p>
        To show the NFA performance alone, I stripped out the 
        matching lines. Predictably, things got a little better.
        </p>
    </aside>
</section>

<section id="scheduled-task">
    <h2>Summarizing Scheduled Tasks</h2>

    <pre class="fragment">[2018-02-15 11:39:25,728] TRACE Beginning execution
    of scheduled task 'isr-expiration'. (kafka.utils.KafkaScheduler)</pre>

    <pre class="fragment">/.* scheduled task '(.+?)'.*/</pre>


    <aside class="notes">
        <p>
        Kafka also has a lot of scheduled tasks that it executes.
        I wanted to learn more about them, and figure out which
        tasks were the most common. I decided to use this regular
        expression to extract the task names and build a histogram
        of them.
        </p>
    </aside>
</section>

<section id="scheduled-graph">
    <h2>Performance on 25M of Input</h2>

    <img height="400px" src="./resources/named.png">

    <aside class="notes">
        <p>The improvement here was even better. This is probably
           because many more of the input lines had to do with
           scheduled tasks. This is a pretty common event.
        </p>
    </aside>
</section>

<section id="scheduled-graph">
    <h2>Joint Performance on 25M of Input</h2>

    <img height="400px" src="./resources/append-named.png">

    <aside class="notes">
        <p>And these were the results when I ran both analysis 
           tasks at once.
        </p>
    </aside>
</section>

</section> <!-- case-study -->

<section id="crates-io">
    <h2>How often are these optimizations useful?</h2>

    <ul>
        <li class="fragment">82.8% of crates.io regex could be optimized.</li>
        <li class="fragment">74.1% could have a skip.</li>
        <li class="fragment"><span
            class="fragment highlight-red">
            15.0% could make use of a scan.</span></li>
    <ul>

    <aside class="notes">
        So far I've shown you examples that I had some hand
        in creating, so now I want to talk about how frequenty
        these optimizations are useful in practice. To get a
        picture of how applicable these optimizations are
        I scraped crates.io, the rust library repository.
        In my opinion, this 15% number is the weakest result
        I have to show you. Scans have the biggest practical
        potential to improve things. It's worth noting that the
        crate.io sample was probably biased towards more complex
        library style crates. I think these numbers are decent, but
        not paradigm shattering.
    </aside>
</section>

<section id="related-work">
    <h2>Related Work</h2>

    <ul>
        <li class="fragment">Gallant - rust-lang/regex</li>
        <li class="fragment">Cox - RE2</li>
        <li class="fragment">Chivers - Previews</li>
        <li class="fragment">... many more</li>
    <ul>

    <aside class="notes">
        I benefited from a very rich literature and implementation
        tradition. My work would not have been nearly so pleasant
        if Andrew Gallant had not done some serious trailblazing
        of typesafe regex implementation. Most of the ideas in
        rust-lang/regex were first explored by Russ Cox in his
        re2 regex engine. Howard Chivers has a series of regex
        optimizations to NFA simulations based off of extracting 
        "preview DFAs" from regular expressions that bear a
        few similarities to the optimizations I just told you
        about. Our implementations are complimentary. I don't
        want to spend too much time on this, but if you read
        my thesis there is a lot more.
    </aside>
</section>

<section id="future-work">
    <h2>Future Work</h2>

    <ul>
        <li class="fragment">Formalize Correctness Proof.</li>
        <li class="fragment">PADS integration.</li>
        <li class="fragment">Add support for unicode and case folding.</li>
        <li class="fragment">Cost Model.</li>
    <ul>

    <aside class="notes">
        If I had more time to work on this there are a few different
        areas that I would want to work on. Right now my correctness
        criterion is a set of unit tests and some informal reasoning;
        I'd like more time to firm that up. This was originally conceived
        in terms of improving PADS, so extending the ideas here to
        apply to PADS is a natural next step. I omitted a few features
        supported by rust-lang/regex for development speed. It would
        be nice to fill those holes. Right now I just have some
        precedence rules for the optimizations I perform, an
        actual cost model seems like a worthwhile thing to have.
    </aside>
</section>

<section>
    <h2>Takeaways</h2>

    <ul>
        <li class="fragment">Current regex engines decide languages once too many times.</li>
        <li class="fragment">Parsing need not include deciding.</li>
        <li class="fragment">NFA Simulation &lt;&lt; DFA &lt;&lt; Substring Search</li>
    <ul>

    <aside class="notes">
        If you walk away from this talk with just three takeaways I want
        you to remember that current engines decide regex twice when they
        have to parse something, parsing a regex does not need to include
        deciding it, and this relationship is a good rule of thumb for
        thinking about regex performance.
    </aside>
</section>

<!-- --------------------------------------------------------------------
                             REVEAL.JS FOOTER 
     -------------------------------------------------------------------- -->

			</div>
		</div>


		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>
		<script>
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js',
                      async: true,
                      callback: function() { hljs.initHighlightingOnLoad(); }
                    },
                    { src: 'https://d3js.org/d3.v4.min.js' },
                    { src: 'node_modules/reveal.js-d3js/d3js.js' }
				]
			});
		</script>
	</body>
</html>
